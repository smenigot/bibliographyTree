{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuBYK9UksLd3"
      },
      "source": [
        "## Installation du cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKn5_kKzHE9i"
      },
      "outputs": [],
      "source": [
        "pip install diskcache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXex1dvCsYrQ"
      },
      "source": [
        "## Défintion des fonctions pour les requetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po_RpTIlHAJF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import diskcache as dc\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.cm as cm\n",
        "import math\n",
        "\n",
        "# Initialiser le cache\n",
        "cache = dc.Cache('cache_dir')\n",
        "\n",
        "def get_article_metadata(doi, min_year=None):\n",
        "    \"\"\"\n",
        "    Récupère les métadonnées d'un article donné à partir de l'API Crossref.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.crossref.org/works/{doi}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()['message']\n",
        "\n",
        "        # Gérer les cas où le titre est absent ou vide\n",
        "        title = data.get('title', ['Unknown Title'])\n",
        "        title = title[0] if title else 'Unknown Title'\n",
        "\n",
        "        authors = [\n",
        "            {\n",
        "                \"given\": author.get(\"given\", \"\"),\n",
        "                \"family\": author.get(\"family\", \"\"),\n",
        "            }\n",
        "            for author in data.get(\"author\", [])\n",
        "        ]\n",
        "\n",
        "        journal = data.get('container-title', ['Unknown Journal'])[0] if data.get('container-title') else data.get('publisher', '')\n",
        "        year = data.get('published-print', {}).get('date-parts', [[None]])[0][0]\n",
        "        if year is None:\n",
        "            return None\n",
        "\n",
        "        # Vérification de l'année minimale\n",
        "        if min_year and isinstance(year, int) and year < min_year:\n",
        "            #print(f\"Article rejeté (publié avant {min_year}) : {doi}\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            \"doi\": doi,\n",
        "            \"title\": title,\n",
        "            \"authors\": authors,\n",
        "            \"journal\": journal,\n",
        "            \"year\": year,\n",
        "        }\n",
        "    else:\n",
        "        print(f\"Erreur : Impossible de récupérer les données pour le DOI {doi}\")\n",
        "        return None\n",
        "\n",
        "def get_related_dois(doi, min_year, direction=\"references\"):\n",
        "    \"\"\"\n",
        "    Récupère les articles cités ou citant un article donné via OpenCitations.\n",
        "    \"\"\"\n",
        "    base_url = \"https://opencitations.net/index/api/v1/\" + (\"citations/\" if direction == \"citations\" else \"references/\")\n",
        "    url = f\"{base_url}{doi}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return [item.get('cited', '') if direction == \"references\" else item.get('citing', '') for item in response.json()]\n",
        "    else:\n",
        "        print(f\"Erreur lors de la récupération des {direction} pour le DOI {doi}: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def get_article_metadata_cached(doi, min_year):\n",
        "    if doi in cache:\n",
        "        print(f\"Article déjà interrogé {doi}\")  # Afficher que l'article est récupéré depuis le cache\n",
        "        return cache[doi]\n",
        "    data = get_article_metadata(doi, min_year)\n",
        "    if data:\n",
        "        print(f\"Nouvel article {doi}\")  # Afficher que l'article est récupéré via l'API\n",
        "        cache[doi] = data\n",
        "    return data\n",
        "\n",
        "def get_related_dois_cached(doi, min_year, direction=\"references\"):\n",
        "    cache_key = f\"{doi}_{direction}\"\n",
        "    if cache_key in cache:\n",
        "        return cache[cache_key]\n",
        "    related_dois = get_related_dois(doi, min_year, direction)\n",
        "    cache[cache_key] = related_dois\n",
        "    return related_dois\n",
        "\n",
        "\n",
        "def process_article(doi, depth=3, min_year=1965):\n",
        "    \"\"\"\n",
        "    Récupère les métadonnées et construit un graphe avec les articles liés.\n",
        "    Exclut les articles sans année ou sans nom de journal.\n",
        "    Utilise une approche itérative pour éviter la récursion.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Utilisation d'une pile pour gérer les nœuds à traiter avec leur profondeur\n",
        "    stack = [(doi, 0, True)]  # (doi, current_depth, is_initial_article)\n",
        "\n",
        "    # Tant qu'il y a des nœuds à traiter dans la pile\n",
        "    while stack:\n",
        "        # Récupérer le nœud actuel et sa profondeur\n",
        "        current_doi, current_depth, is_initial_article = stack.pop()\n",
        "\n",
        "        # Vérifier si la profondeur a été atteinte\n",
        "        if current_depth > depth:\n",
        "            continue\n",
        "\n",
        "        # Récupérer les métadonnées avec mise en cache\n",
        "        article_data = get_article_metadata_cached(current_doi, min_year)\n",
        "\n",
        "        if article_data:\n",
        "          # Assurez-vous que les données de l'article sont valides\n",
        "          title = article_data.get(\"title\", \"\")\n",
        "          if title and article_data.get(\"journal\"):\n",
        "              # Ajouter le nœud au graphe avec les métadonnées\n",
        "              print(f\"Ajout du nœud : {current_doi}\")\n",
        "              if current_doi not in G.nodes:\n",
        "                  G.add_node(current_doi)\n",
        "\n",
        "              # Ajouter les métadonnées au nœud\n",
        "              G.nodes[current_doi][\"title\"] = article_data.get(\"title\", \"Unknown Title\")\n",
        "              G.nodes[current_doi][\"authors\"] = article_data.get(\"authors\", [])\n",
        "              G.nodes[current_doi][\"year\"] = article_data.get(\"year\", \"Unknown Year\")\n",
        "              G.nodes[current_doi][\"journal\"] = article_data.get(\"journal\", \"Unknown Journal\")\n",
        "\n",
        "          # Ajouter les articles référencés et cités\n",
        "          if is_initial_article:\n",
        "              # Pour l'article de départ, on ajoute à la pile les DOIs citants ET référencés\n",
        "              referenced_dois = get_related_dois_cached(current_doi, min_year, direction=\"references\")\n",
        "              citing_dois = get_related_dois_cached(current_doi, min_year, direction=\"citations\")\n",
        "\n",
        "              # Ajouter les arêtes (relations entre les nœuds)\n",
        "              for ref_doi in referenced_dois:\n",
        "                  if ref_doi and ref_doi not in G.nodes:  # Vérifier l'existence avant l'ajout\n",
        "                      G.add_edge(current_doi, ref_doi)\n",
        "                      stack.append((ref_doi, current_depth + 1, False))  # False, ce n'est plus l'article initial\n",
        "\n",
        "              for cite_doi in citing_dois:\n",
        "                  if cite_doi and cite_doi not in G.nodes:  # Vérifier l'existence avant l'ajout\n",
        "                      G.add_edge(cite_doi, current_doi)\n",
        "                      stack.append((cite_doi, current_depth + 1, False))  # False, ce n'est plus l'article initial\n",
        "\n",
        "          else:\n",
        "              # Pour les articles référencés ou citants, on ajoute seulement les articles référencés\n",
        "              referenced_dois = get_related_dois_cached(current_doi, min_year, direction=\"references\")\n",
        "\n",
        "              for ref_doi in referenced_dois:\n",
        "                  if ref_doi and ref_doi not in G.nodes:  # Vérifier l'existence avant l'ajout\n",
        "                      G.add_edge(current_doi, ref_doi)\n",
        "                      stack.append((ref_doi, current_depth + 1, False))  # False, ce n'est plus l'article initial\n",
        "\n",
        "    return G\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph(graph, root_doi, max_generation=2, show_labels=True, min_year_graph=1990):\n",
        "    \"\"\"\n",
        "    Affiche le graphe avec :\n",
        "    - des couleurs pour chaque année de publication,\n",
        "    - une réduction des nœuds et connexions pour une meilleure lisibilité,\n",
        "    - des labels pour les nœuds les plus significatifs.\n",
        "\n",
        "    Args:\n",
        "    - graph: Le graphe NetworkX représentant les articles et leurs relations.\n",
        "    - root_doi: DOI de l'article central, qui sera mis en évidence.\n",
        "    - max_generation: Nombre de générations à inclure dans le graphe (positives et négatives).\n",
        "    - show_labels: Booléen pour activer ou désactiver l'affichage des labels des nœuds.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib import cm\n",
        "    from matplotlib import colors as mcolors\n",
        "\n",
        "    # Filtrer les nœuds ayant des métadonnées\n",
        "    nodes_with_metadata = [node for node in graph.nodes if graph.nodes[node]]\n",
        "    subgraph = graph.subgraph(nodes_with_metadata)  # Créer un sous-graphe\n",
        "\n",
        "    # Vérifier si le root_doi est présent dans le sous-graphe\n",
        "    if root_doi not in subgraph:\n",
        "        raise ValueError(f\"L'article central {root_doi} n'est pas dans les nœuds avec métadonnées.\")\n",
        "\n",
        "    # Calculer les générations relatives\n",
        "    generations = {}\n",
        "    for node in subgraph.nodes:\n",
        "        try:\n",
        "            if nx.has_path(subgraph, root_doi, node):\n",
        "                generations[node] = nx.shortest_path_length(subgraph, root_doi, node)  # Génération +\n",
        "            elif nx.has_path(subgraph, node, root_doi):\n",
        "                generations[node] = -nx.shortest_path_length(subgraph, node, root_doi)  # Génération -\n",
        "            else:\n",
        "                generations[node] = float('inf')  # Pas de chemin\n",
        "        except nx.NetworkXNoPath:\n",
        "            generations[node] = float('inf')\n",
        "\n",
        "    # Filtrer les nœuds hors du max_generation\n",
        "    filtered_nodes = [node for node, gen in generations.items() if abs(gen) <= max_generation]\n",
        "    subgraph = subgraph.subgraph(filtered_nodes)  # Réduire le graphe aux nœuds filtrés\n",
        "\n",
        "    # Redéfinir les générations pour le sous-graphe\n",
        "    generations = {node: gen for node, gen in generations.items() if node in subgraph}\n",
        "\n",
        "    # EXTRAIT : Couleur par année de publication\n",
        "    years = []\n",
        "    for node in subgraph.nodes:\n",
        "        year = graph.nodes[node].get('year', None)\n",
        "        try:\n",
        "            # Si l'année est valide (un entier), on l'ajoute\n",
        "            if year and year != 'Unknown':\n",
        "                years.append(int(year))  # Ajouter l'année à la liste\n",
        "            else:\n",
        "                years.append(min_year_graph)  # Remplacer 'Unknown' ou l'absence d'année par une valeur par défaut\n",
        "        except ValueError:\n",
        "            years.append(min_year_graph)  # Si l'année n'est pas un nombre valide, on l'assigne à -1\n",
        "\n",
        "    # Normalisation de l'année pour utiliser une colormap\n",
        "    min_year = min(years)\n",
        "    max_year = max(years)\n",
        "    norm = mcolors.Normalize(vmin=min_year, vmax=max_year)\n",
        "    cmap = plt.get_cmap(\"viridis\")  # Utiliser une colormap qui varie avec l'année\n",
        "\n",
        "    # Colorer les nœuds par année de publication\n",
        "    node_colors = []\n",
        "    for node in subgraph.nodes:\n",
        "        year = graph.nodes[node].get(\"year\", None)\n",
        "        try:\n",
        "            # Si l'année est valide, on l'ajoute à la colormap\n",
        "            year_value = int(year) if year not in [\"Unknown\", None] else -1\n",
        "            node_colors.append(cmap(norm(year_value)))\n",
        "        except ValueError:\n",
        "            # Si l'année n'est pas valide, on met la couleur grise\n",
        "            node_colors.append(\"gray\")\n",
        "\n",
        "    # Calculer les tailles des nœuds proportionnelles au degré\n",
        "    degrees = dict(subgraph.degree())\n",
        "    node_sizes = [50 + degrees[node] * 200 for node in subgraph.nodes]\n",
        "\n",
        "    # Mise en évidence de l'article central (root_doi)\n",
        "    root_size = 300  # Taille spéciale pour l'article central\n",
        "    node_sizes = [\n",
        "        root_size if node == root_doi else size\n",
        "        for node, size in zip(subgraph.nodes, node_sizes)\n",
        "    ]\n",
        "    node_colors = [\n",
        "        \"gold\" if node == root_doi else color\n",
        "        for node, color in zip(subgraph.nodes, node_colors)\n",
        "    ]\n",
        "\n",
        "    # Créer un dictionnaire de labels : seulement pour les gros nœuds\n",
        "    def get_label(data):\n",
        "        first_author = \"Unknown\"\n",
        "        year = \"Unknown Year\"\n",
        "        if \"authors\" in data and data[\"authors\"]:\n",
        "            first_author = data[\"authors\"][0].get(\"family\", \"Unknown\")\n",
        "        if \"year\" in data:\n",
        "            year = data[\"year\"]\n",
        "        return f\"{first_author} ({year})\"\n",
        "\n",
        "    labels = {\n",
        "        node: get_label(subgraph.nodes[node]) if degrees[node] > 2 else \"\"\n",
        "        for node in subgraph.nodes\n",
        "    } if show_labels else None\n",
        "\n",
        "    # Obtenir les positions des nœuds\n",
        "    pos = nx.spring_layout(subgraph, seed=42, k=0.3)\n",
        "\n",
        "    # Tracer le graphe\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    nx.draw(\n",
        "        subgraph,\n",
        "        pos,\n",
        "        with_labels=True,\n",
        "        node_size=node_sizes,\n",
        "        node_color=node_colors,\n",
        "        edge_color=\"gray\",\n",
        "        alpha=0.7,\n",
        "        font_size=10 if show_labels else 0,\n",
        "        labels=labels\n",
        "    )\n",
        "\n",
        "    # Ajouter une colorbar pour les années de publication\n",
        "    sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    cbar = plt.colorbar(sm, ax=plt.gca())\n",
        "    cbar.set_label(\"Année de publication\")\n",
        "\n",
        "    # Titre et affichage\n",
        "    plt.title(f\"Graphe des articles liés à {root_doi}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "i-DPsPohdNbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def save_graph(graph, output_file=\"graph.pkl\"):\n",
        "    \"\"\"\n",
        "    Sauvegarde un graphe NetworkX dans un fichier au format pickle.\n",
        "\n",
        "    :param graph: Le graphe NetworkX à sauvegarder.\n",
        "    :param output_file: Chemin du fichier dans lequel enregistrer le graphe.\n",
        "    \"\"\"\n",
        "    with open(output_file, \"wb\") as f:\n",
        "        pickle.dump(graph, f)\n",
        "    print(f\"Le graphe a été sauvegardé dans {output_file}\")\n",
        "\n",
        "def load_graph(input_file=\"graph.pkl\"):\n",
        "    \"\"\"\n",
        "    Charge un graphe NetworkX à partir d'un fichier pickle.\n",
        "\n",
        "    :param input_file: Chemin du fichier pickle contenant le graphe.\n",
        "    :return: Le graphe NetworkX chargé.\n",
        "    \"\"\"\n",
        "    with open(input_file, \"rb\") as f:\n",
        "        graph = pickle.load(f)\n",
        "    print(f\"Le graphe a été chargé depuis {input_file}\")\n",
        "    return graph\n"
      ],
      "metadata": {
        "id": "PWEarDHLxvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_bibtex(graph, output_file=\"bibliography.bib\"):\n",
        "    \"\"\"\n",
        "    Enregistre les métadonnées des nœuds du graphe au format BibTeX, en incluant le nombre de connexions entrantes\n",
        "    et sortantes entre les nœuds ayant des métadonnées.\n",
        "\n",
        "    :param graph: Le graphe contenant les nœuds avec les métadonnées.\n",
        "    :param output_file: Le chemin du fichier dans lequel enregistrer les entrées BibTeX.\n",
        "    \"\"\"\n",
        "    # Filtrer les nœuds avec métadonnées\n",
        "    nodes_with_metadata = [node for node in graph.nodes if graph.nodes[node]]\n",
        "    subgraph = graph.subgraph(nodes_with_metadata)  # Créer un sous-graphe\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for node in subgraph.nodes:\n",
        "            try:\n",
        "                data = subgraph.nodes[node]  # Accède aux métadonnées du nœud\n",
        "\n",
        "                # Extraire les informations\n",
        "                doi = node\n",
        "                title = data.get(\"title\", \"Unknown Title\")\n",
        "                authors = data.get(\"authors\", [])\n",
        "                authors_str = \" and \".join([\n",
        "                    f\"{a['family']}, {a['given'][0]}.\" if 'family' in a and 'given' in a and a['given'] else \"Unknown Author\"\n",
        "                    for a in authors\n",
        "                ]) if authors else \"Unknown Author\"\n",
        "\n",
        "                year = data.get(\"year\", \"Unknown Year\")\n",
        "                journal = data.get(\"journal\", \"Unknown Journal\")\n",
        "\n",
        "                # Vérifier si le titre et le DOI sont valides\n",
        "                if title == \"Unknown Title\" or not doi:\n",
        "                    print(f\"Article invalide : titre ou DOI manquant pour {doi}\")\n",
        "                    continue\n",
        "\n",
        "                # Calculer les connexions entrantes et sortantes\n",
        "                incoming_connections = subgraph.in_degree(node)\n",
        "                outgoing_connections = subgraph.out_degree(node)\n",
        "\n",
        "                # Construire l'entrée BibTeX\n",
        "                entry = f\"\"\"@article{{{doi.replace('/', '_')},\n",
        "                  title = {{{title}}},\n",
        "                  author = {{{authors_str}}},\n",
        "                  year = {{{year}}},\n",
        "                  journal = {{{journal}}},\n",
        "                  doi = {{{doi}}},\n",
        "                  incoming = {{{incoming_connections}}},\n",
        "                  outgoing = {{{outgoing_connections}}}\n",
        "                }}\\n\"\"\"\n",
        "\n",
        "                # Écrire l'entrée dans le fichier\n",
        "                f.write(entry)\n",
        "            except Exception as e:\n",
        "                # Afficher un message d'erreur et continuer avec le prochain nœud\n",
        "                print(f\"Erreur lors de l'écriture de l'article {node}: {e}\")\n",
        "\n",
        "    print(f\"Les métadonnées avec connexions ont été enregistrées dans le fichier {output_file}\")"
      ],
      "metadata": {
        "id": "XBKxBqE-dT6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojpF-CKlsPZh"
      },
      "source": [
        "## Choix de l'article à étudier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuLZZuMzsUlD"
      },
      "outputs": [],
      "source": [
        "doi = \"10.1016/j.bspc.2019.101811\"  # Exemple de DOI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPIlnM5psU6f"
      },
      "source": [
        "## Choix des limites d'analyse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lmupuUssy71"
      },
      "outputs": [],
      "source": [
        "min_year = 1990\n",
        "max_depth = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEc9Y4tys0Dp"
      },
      "source": [
        "## Création de l'arbre"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Vérifier si un fichier de graphe existe déjà\n",
        "graph_file = \"graph.pkl\"\n",
        "reload_graph = False  # Passez à True si vous souhaitez recharger un graphe existant\n",
        "\n",
        "if reload_graph and os.path.exists(graph_file):\n",
        "    # Charger un graphe existant\n",
        "    graph = load_graph(graph_file)\n",
        "else:\n",
        "    # Traiter l'article et construire le graphe\n",
        "    graph = process_article(doi, depth=max_depth, min_year=min_year)\n",
        "    # Sauvegarder le graphe\n",
        "    save_graph(graph, graph_file)\n",
        "print(\"Graph OK\")\n",
        "\n",
        "# Visualiser le graphe\n",
        "visualize_graph(graph, doi, show_labels=True, min_year_graph=min_year)\n",
        "print(\"Print Graph OK\")\n",
        "\n",
        "# Sauvegarder le graphe en BibTeX\n",
        "save_bibtex(graph)\n",
        "print(\"Save Data OK\")\n"
      ],
      "metadata": {
        "id": "gg4ShRpRx6Kr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iuBYK9UksLd3"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}